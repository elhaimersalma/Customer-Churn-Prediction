# -*- coding: utf-8 -*-
"""ML Project final.ipy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10NG6l2Cj8Sr6ZF1Bq2FoxnHjIcrtqPXY

###        **Predictive Analysis on Customer Churn in Banking Industry**
"""

from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

"""Importing Librairies

"""

import pandas as pd
dataset = pd.read_csv('churn_prediction.csv')

import pandas as pd
dataset = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/churn_prediction_new (3).csv')

#Data manipulation
import numpy as np
import pandas as pd

#Models
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model  import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

#Preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

#Cross_validation
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import validation_curve
from sklearn.model_selection import cross_validate
#hyperparametre tuning

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

#Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn import metrics

#oversampling
from imblearn.over_sampling import SMOTE
from imblearn import over_sampling

from sklearn.decomposition import PCA

#Model evaluation
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, classification_report

#Plot
import seaborn as sns
from plotly.graph_objs import Sankey
import itertools
import matplotlib.pyplot as plt
!pip install shap
import shap

"""**OVERVIEW**"""

dataset

dataset.info()

dataset.shape

dataset.describe()

dataset.columns

"""## **Visualization**"""

def pdes(feature,x,y):
    plt.figure(figsize=(x,y))
    fig_churn = sns.countplot(x=feature, hue='churn', data = dataset, palette='RdBu')
    fig_churn.bar_label(fig_churn.containers[0])
    fig_churn.bar_label(fig_churn.containers[1])
    print(dataset[feature].value_counts(normalize = True))

"""Churn Bivariant features

"""

df_churn = dataset.loc[dataset.churn==1]
df_non_churn = dataset.loc[dataset.churn==0]

sns.countplot(x="churn", data=dataset ,palette='RdBu')
plt.title('number of churn attempts')
plt.show()
print('number of non churn examples: ',df_non_churn.churn.count())
print('number of churn examples: ',df_churn.churn.count())

plt.figure(figsize=(5,4))
palette_color = sns.color_palette('RdBu')
labels =['churn: No', 'churn: Yes']
values = dataset['churn'].value_counts()
labels_gender = ['F', 'M', 'F', 'M']
values_gender = dataset.groupby(['churn', 'gender'])['gender'].value_counts()
width = 3
plt.pie(values, autopct='%1.1f%%',
       colors=palette_color, labels=labels, radius=10, startangle=90, pctdistance=1.2, labeldistance=1.0,
       wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'white' })
plt.pie(values_gender, labels=labels_gender, autopct='%1.1f%%',
       colors=sns.color_palette('RdBu_r'), radius=10 - width, startangle=90,
       wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'white' })

centre_circle = plt.Circle((0,0),5,color='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn by Gender: Male(M), Female(F)', fontsize=14)
plt.axis('equal')
plt.tight_layout()
plt.show()

bins = [0, 18, 25, 35, 50, 65, 100]     # classes d'âge
labels = ['0-18', '18-25', '25-35', '35-50', '50-65', '65+']    # étiquettes pour chaque classe d'âge
dataset['age_group'] = pd.cut(dataset['age'], bins=bins, labels=labels)
dataset['age_group'] = dataset['age_group'].cat.reorder_categories(labels, ordered=True)

# Tracer les graphiques
fig, (ax1, ax2)= plt.subplots(nrows=1, ncols=2, figsize=(15, 4))
sns.countplot(data=dataset, x='age_group', hue='churn', order=dataset['age_group'].value_counts().index, palette='RdBu', ax=ax1)
sns.countplot(data=dataset, x='occupation', hue='churn', order=dataset['occupation'].value_counts().index, palette='RdBu', ax=ax2)

fig.tight_layout()
plt.show()

tenure_churn_no = dataset[dataset.churn==0].vintage
tenure_churn_yes = dataset[dataset.churn==1].vintage

plt.xlabel("vintage")
plt.ylabel("number of customers")
plt.title('customer churn prediction visualisation')

colors = plt.cm.RdBu(np.linspace(0, 1, 2))
plt.hist([tenure_churn_yes, tenure_churn_no], color=colors, label=['ChurnYes','ChurnNo'])
plt.legend()
fig, (ax1)= plt.subplots(nrows=1, ncols=1, figsize=(20, 10))
sns.countplot(data=dataset, x='dependents', hue='churn', order=dataset['dependents'].value_counts().index, palette='RdBu', ax=ax1)

fig.tight_layout()
plt.show()

"""Boxplot by churn"""

fig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8),(ax9,ax10)) = plt.subplots(ncols=2, nrows=5, figsize = (20, 20))


ax = dataset['current_balance'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax1)
ax1.set_title(" Balance as of today")

ax = dataset['previous_month_end_balance'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax2)
ax2.set_title("  End of Month Balance of previous month -")

ax = dataset['average_monthly_balance_prevQ'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax3)
ax3.set_title("Average monthly balances (AMB) in Previous Quarter")

ax = dataset['average_monthly_balance_prevQ2'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax4)
ax4.set_title("Average monthly balances (AMB) in previous to the previous quarter -")

ax = dataset['current_month_credit'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax5)
ax5.set_title(" Total Credit Amount current month -")

ax = dataset['previous_month_credit'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax6)
ax6.set_title("Total Credit Amount previous month -")

ax = dataset['current_month_debit'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax7)
ax7.set_title("  Total Credit Amount previous month ")

ax = dataset['previous_month_debit'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax8)
ax8.set_title(" - Total Debit Amount previous month-")

ax = dataset['current_month_balance'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax9)
ax9.set_title(" Average Balance of current month -")

ax = dataset['previous_month_balance'].value_counts().sort_index().plot.hist(color = '#2f528f', ax = ax10)
ax10.set_title(" Average Balance of previous month")
plt.show()

"""dropping columns"""

dataset= dataset.drop(['customer_id' , 'current_month_balance', 'previous_month_end_balance', 'current_month_debit' , 'average_monthly_balance_prevQ' , 'previous_month_balance' ] , axis= 'columns')

dataset.columns

dataset.info()

num_vars = ['vintage', 'age','dependents','city','customer_nw_category', 'days_since_last_transaction', 'average_monthly_balance_prevQ2','current_balance', 'current_month_credit', 'previous_month_credit' , 'previous_month_debit' ]

fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20, 10))
axs = axs.flatten()

for i, var in enumerate(num_vars):
    sns.boxplot(x=var, data=dataset, ax=axs[i])

fig.tight_layout()

plt.show()

num_vars = ['vintage', 'age','dependents','city','customer_nw_category', 'days_since_last_transaction', 'average_monthly_balance_prevQ2','current_balance', 'current_month_credit', 'previous_month_credit' , 'previous_month_debit' ]

dataset['zscore1'] = ( dataset.vintage- dataset.vintage.mean() ) / dataset.vintage.std()
dataset['zscore2'] = ( dataset.dependents- dataset.dependents.mean() ) / dataset.dependents.std()
dataset['zscore3'] = ( dataset.days_since_last_transaction- dataset.days_since_last_transaction.mean() ) / dataset.days_since_last_transaction.std()
dataset['zscore4'] = ( dataset.average_monthly_balance_prevQ2- dataset.average_monthly_balance_prevQ2.mean() ) / dataset.average_monthly_balance_prevQ2.std()
dataset['zscore5'] = ( dataset.current_balance- dataset.current_balance.mean() ) / dataset.current_balance.std()
dataset['zscore6'] = ( dataset.current_month_credit- dataset.current_month_credit.mean() ) / dataset.current_month_credit.std()
dataset['zscore7'] = ( dataset.previous_month_debit- dataset.previous_month_debit.mean() ) / dataset.previous_month_debit.std()

df_no_outliers = dataset[(dataset.zscore1>-3) & (dataset.zscore1<3)]
df_no_outliers = dataset[(dataset.zscore2>-3) & (dataset.zscore2<3)]
df_no_outliers = dataset[(dataset.zscore3>-3) & (dataset.zscore3<3)]
df_no_outliers = dataset[(dataset.zscore4>-3) & (dataset.zscore4<3)]
df_no_outliers = dataset[(dataset.zscore5>-3) & (dataset.zscore5<3)]
df_no_outliers = dataset[(dataset.zscore6>-3) & (dataset.zscore6<3)]
df_no_outliers = dataset[(dataset.zscore7>-3) & (dataset.zscore7<3)]
dataset = df_no_outliers.copy()
dataset.head()

dataset= dataset.drop(['zscore1' ,'zscore2','zscore3','zscore4',	'zscore5'	,'zscore6',	'zscore7'] , axis = 'columns')

plt.figure(figsize=(13, 8))
sns.heatmap(dataset.corr(), cmap='Blues', annot=True, fmt='.2f')

"""# Feature engineering

## **Data Preprocessing**

**Checking missing values**
"""

dataset.isnull().sum()

imputer = SimpleImputer(strategy='most_frequent')
dataset['gender'] = imputer.fit_transform(dataset[['gender']])

imputer = SimpleImputer(strategy='most_frequent')
dataset['occupation'] = imputer.fit_transform(dataset[['occupation']])

dataset['dependents']= dataset['dependents'].fillna(dataset['dependents'].mean())
dataset['city']= dataset['city'].fillna(dataset['city'].mean())
dataset['days_since_last_transaction']= dataset['days_since_last_transaction'].fillna(dataset['days_since_last_transaction'].mean())

dataset.isnull().sum()

"""**Check Duplicated Data**"""

dataset.duplicated().sum()

def checkBalance(dataset, target_col):
    totalPerClass = dataset[target_col].value_counts()
    total = totalPerClass.sum()

    for idx, classSum in totalPerClass.items():
        print(f'{classSum} {idx} {classSum/total*100:0.1f}% ')

print("Churn dataset")
checkBalance(dataset, 'churn')

"""**Extracting the data and Feature Encoding**"""

dataset.columns

dataset.info()

dataset.shape

dataset.info()
X=dataset.iloc[:,0:14].values
y=dataset.iloc[:,14].values

labelencoder_X_1=LabelEncoder()
X[:,2]=labelencoder_X_1.fit_transform(X[:,2])
labelencoder_X_2=LabelEncoder()
X[:,4]=labelencoder_X_1.fit_transform(X[:,4])

column_transformer = ColumnTransformer(
    [('onehot', OneHotEncoder(), [4])],  # one-hot encode column 1
    remainder='passthrough')  # passthrough all other columns


X=column_transformer.fit_transform(X)
X=X[:,4:]



X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=42)

print('X_train',X_train.shape)
print('y_train',y_train.shape)
print('X_test',X_test.shape)
print('y_test',y_test.shape)

scaler = StandardScaler()
# fit the scaler on the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)
# transform the testing data using the scaler fitted on the training data
X_test_scaled = scaler.transform(X_test)

X_train_scaled

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X_train_scaled)
principalDf = pd.DataFrame(data = principalComponents
                , columns = ['principal component 1', 'principal component 2'])
finalDf = pd.concat([principalDf, dataset[['churn']]], axis = 1)
finalDf.head()

fig = plt.figure(figsize = (5,5))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 30)
targets = [0,1]
colors = ['lightblue', 'pink']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['churn'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

"""**MODELLING**

**Algorithms**
"""

lr = LogisticRegression(random_state=42,max_iter=10000)
svc = SVC(random_state=42,probability=True)
rf= RandomForestClassifier(random_state=42)
knn=KNeighborsClassifier()
xgb_clf = XGBClassifier(random_state=42)

"""**Smote**

Since the dataset is imbalanced so we are going to use the SMOTE or Synthetic Minority Oversampling Technique is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. One approach to addressing imbalanced datasets is to oversample the minority class such as SMOTE. The simplest approach by SMOTE involves duplicating examples in the minority class without add any new information to the model.
"""

X_train_smote, y_train_smote = over_sampling.SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)
print(X_train_smote.shape, y_train_smote.shape)

def eval_classification(model):
  # Prediction
    y_pred = model.predict(X_test_scaled)
    y_pred_train = model.predict(X_train_smote)
    y_pred_proba = model.predict_proba(X_test_scaled)
    y_pred_proba_train = model.predict_proba(X_train_smote)

  # Test Score
    print('Scoring')
    print('Accuracy (Test): %.2f' % accuracy_score(y_test, y_pred))
    print('Accuracy (Train): %.2f' % accuracy_score(y_train_smote, y_pred_train))
    print('Precision (Test): %.2f' % precision_score(y_test, y_pred))
    print('Precision (Train): %.2f' % precision_score(y_train_smote, y_pred_train))
    print('Recall (Test): %.2f' % recall_score(y_test, y_pred))
    print('Recall (Train): %.2f' % recall_score(y_train_smote, y_pred_train))
    print('F1 (Test): %.2f' % f1_score(y_test, y_pred))
    print('F1 (Train): %.2f' % f1_score(y_train_smote, y_pred_train))
    print('AUC (Test Proba): %.2f' % roc_auc_score(y_test, y_pred_proba[:,1]))
    print('AUC (Train Proba): %.2f' % roc_auc_score(y_train_smote, y_pred_proba_train[:,1]))
    print('Test Score: %.2f' % model.score(X_test_scaled, y_test))
    print('Train Score: %.2f' % model.score(X_train_smote, y_train_smote))

# Model Logistic Regression
lr.fit((X_train_smote),( y_train_smote))
# Evaluation
eval_classification(lr)

# Model Random forest
rf.fit(X_train_smote, y_train_smote)

# Evaluation
eval_classification(rf)

# Model SVC
svc.fit(X_train_smote, y_train_smote)

# Evaluation
eval_classification(svc)

# Model KNN
knn.fit(X_train_smote, y_train_smote)

# Evaluation
eval_classification(knn)

# Model XGB
xgb_clf.fit(X_train_smote, y_train_smote)

# Evaluation
eval_classification(xgb_clf)

"""## with cross validation"""

LR_r = cross_validate(lr, X_train_smote, y_train_smote, cv=StratifiedKFold(n_splits=5), scoring='recall', return_train_score=True, n_jobs=-1)
RF_r = cross_validate(rf, X_train_smote, y_train_smote, cv=StratifiedKFold(n_splits=5), scoring='recall', return_train_score=True, n_jobs=-1)
KNN_r = cross_validate(knn, X_train_smote, y_train_smote, cv=StratifiedKFold(n_splits=5), scoring='recall', return_train_score=True, n_jobs=-1)
SVC_r = cross_validate(svc, X_train_smote, y_train_smote, cv=StratifiedKFold(n_splits=5), scoring='recall', return_train_score=True, n_jobs=-1)
XGB_r = cross_validate(xgb_clf, X_train_smote, y_train_smote, cv=StratifiedKFold(n_splits=5), scoring='recall', return_train_score=True, n_jobs=-1)
LR_accuracy_train = LR_r['train_score'].mean()
LR_accuracy_test = LR_r['test_score'].mean()
RF_accuracy_train = RF_r['train_score'].mean()
RF_accuracy_test = RF_r['test_score'].mean()
KNN_accuracy_train = KNN_r['train_score'].mean()
KNN_accuracy_test = KNN_r['test_score'].mean()

SVC_accuracy_train = SVC_r['train_score'].mean()
SVC_accuracy_test = SVC_r['test_score'].mean()
XGB_accuracy_train = XGB_r['train_score'].mean()
XGB_accuracy_test = XGB_r['test_score'].mean()
results = pd.DataFrame([['Logistic Regression', LR_accuracy_train, LR_accuracy_test],
                        ['RF', RF_accuracy_train, RF_accuracy_test],
                        ['KNN', KNN_accuracy_train, KNN_accuracy_test],
                        ['SVC', SVC_accuracy_train, SVC_accuracy_test],
                        ['XGBoost',XGB_accuracy_train, XGB_accuracy_test]],
                        columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

"""## **Hyperparameter Tuning**

## **Random forest**
"""

pipeline = Pipeline([
        ('params', rf)
    ])

param = {'params__max_depth': [4, 5],
         'params__max_features': [ 'sqrt'],
         'params__min_samples_leaf': [3,4,5],
         'params__min_samples_split': [3,4,5]
        }

RF_tune = GridSearchCV(estimator=pipeline,
                          param_grid=param,
                          scoring='roc_auc',
                          cv=3,
                          n_jobs=-1,
                          verbose=1
                          )
RF_tune.fit(X_train_smote, y_train_smote)
eval_classification(RF_tune)

RF_score = cross_validate(RF_tune, X_train_smote, y_train_smote, cv=3, scoring='roc_auc', return_train_score=True)
RF_train_score = RF_score['train_score'].mean()
RF_test_score = RF_score['test_score'].mean()
results = pd.DataFrame([['Random Forest', RF_train_score, RF_test_score]], columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

"""**Model evaluation :random forest**"""

RF_matrix = RF_tune.fit(X_train_smote, y_train_smote)
y_pred = RF_tune.predict(X_test_scaled)
y_pred_train = RF_tune.predict(X_train_smote)
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True)

"""## **KNN**"""

pipeline = Pipeline([
    ('knn', knn)
])


param = {
    'knn__n_neighbors': [3, 5, 7],
    'knn__weights': ['uniform', 'distance'],
    'knn__algorithm': ['auto'],
    'knn__p': [ 20, 50]
}


knn_tune = GridSearchCV(estimator=pipeline,
                        param_grid=param,
                        scoring='roc_auc',
                        n_jobs=-1,
                        cv=3,
                        verbose=1)


knn_tune.fit(X_train_smote, y_train_smote)


eval_classification(knn_tune)

KNN_score = cross_validate(knn_tune, X_train_smote, y_train_smote, cv=3, scoring='roc_auc', return_train_score=True)
KNN_train_score = KNN_score['train_score'].mean()
KNN_test_score = KNN_score['test_score'].mean()
results = pd.DataFrame([['KNN', KNN_train_score, KNN_test_score]], columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

"""**Model Evaluation :KNN**"""

KNN_matrix = knn_tune.fit(X_train_smote, y_train_smote)
y_pred = knn_tune.predict(X_test_scaled)
y_pred_train = knn_tune.predict(X_train_smote)
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True)

#KNN feature importance
importance = knn.kneighbors(X_train_smote, n_neighbors=10, return_distance=False)
#plot of feature importance
plt.figure(figsize=(10, 10))
plt.plot(importance[0], 'o')
plt.xticks(range(X_train_smote.shape[-1]),rotation=90)
plt.ylabel('n_neighbors')
plt.xlabel('features')
plt.title('Feature importance plot')
plt.show()

#effect of neighbors on accuracy
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_smote,y_train_smote)
    train_accuracy[i] = knn.score(X_train_smote, y_train_smote)
    test_accuracy[i] = knn.score(X_test, y_test)
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()

"""## **XGB**"""

pipeline = Pipeline([
        ('params', xgb_clf)
        ])

param = {'params__min_child_weight': [4, 5],
        'params__gamma': [4, 5],
        'params__subsample': [0.8, 1.0],
        'params__colsample_bytree': [0.8, 1.0],
        'params__max_depth': [4, 5]}

XGB_tune = GridSearchCV(estimator=pipeline,
                          param_grid=param,
                          scoring='roc_auc',
                          n_jobs=-1,
                          cv=3,
                          verbose=1)

XGB_tune.fit(X_train_smote, y_train_smote)
predict =XGB_tune.predict(X_test_scaled)

eval_classification(XGB_tune)

dataset.info()

XGB_score = cross_validate(XGB_tune, X_train_smote, y_train_smote, cv=3, scoring='roc_auc', return_train_score=True)
XGB_train_score = XGB_score['train_score'].mean()
XGB_test_score = XGB_score['test_score'].mean()
results = pd.DataFrame([['XGB', XGB_train_score, XGB_test_score]], columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

features = dataset.iloc[:, [8, 9, 11, 15, 13, 14, 5, 16, 18]]
print(features)

"""**Model evaluation: XGB**"""

XGB_matrix = XGB_tune.fit(X_train_smote, y_train_smote)
y_pred = XGB_tune.predict(X_test_scaled)
y_pred_train = XGB_tune.predict(X_train_smote)
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True)

from xgboost import plot_importance
plot_importance(xgb_clf)
plt.show()

#create a pickle file using serialization
import pickle
pickle_out = open("model.pkl","wb")
pickle.dump(XGB_tune, pickle_out)
pickle_out.close()

"""## **Logistic regression**"""

pipeline = Pipeline([
        ('params', lr)
        ])

param = {'params__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
 'params__penalty': ['l1','l2'],
 'params__solver': ['saga']}

LR_tune = GridSearchCV(estimator=pipeline,param_grid=param,scoring='roc_auc',n_jobs=-1,cv=3,verbose=1)

LR_tune.fit(X_train_smote, y_train_smote)

eval_classification(LR_tune)

LR_score = cross_validate(LR_tune, X_train_smote, y_train_smote, cv=3, scoring='roc_auc', return_train_score=True)
LR_train_score = LR_score['train_score'].mean()
LR_test_score = LR_score['test_score'].mean()
results = pd.DataFrame([['LR', LR_train_score, LR_test_score]], columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

best_lr = LR_tune.best_estimator_.named_steps['params']
coefficients = best_lr.coef_[0]

# Afficher l'importance de chaque variable
for i, v in enumerate(coefficients):
    print('Feature %d: %.5f' % (i, v))

# Afficher l'importance relative de chaque variable dans un graphique
import matplotlib.pyplot as plt
plt.bar([x for x in range(len(coefficients))], coefficients)
plt.show()

"""**Model evaluation: LR**"""

LR_matrix = LR_tune.fit(X_train_smote, y_train_smote)
y_pred = LR_tune.predict(X_test_scaled)
y_pred_train = LR_tune.predict(X_train_smote)
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True)

"""## **SVC**"""

pipeline = Pipeline([
        ('params', svc)
    ])

param = {'params__C': [0.1, 10, 100],
         'params__gamma':  [1, 0.1, 0.001],
         'params__kernel': ['linear','rbf'],
        }

SVC_tune = GridSearchCV(estimator=pipeline,
                          param_grid=param,
                          scoring='roc_auc',
                          cv=3,
                          n_jobs=-1,
                          verbose=1
                          )

SVC_tune.fit(X_train_smote, y_train_smote)

eval_classification(SVC_tune)

SVC_score = cross_validate(SVC_tune, X_train_smote, y_train_smote, cv=3, scoring='roc_auc', return_train_score=True)
SVC_train_score = SVC_score['train_score'].mean()
SVC_test_score = SVC_score['test_score'].mean()
results = pd.DataFrame([['SVC', SVC_train_score, SVC_test_score]], columns = ['Models', 'Training Score', 'Testing Score'])
results.sort_values(by=['Training Score', 'Testing Score'], ascending=False)

"""**Model evaluation:svc**"""

SVC_matrix = SVCtune.fit(X_train_smote, y_train_smote)
y_pred = SVC_tune.predict(X_test_scaled)
y_pred_train = SVC_tune.predict(X_train_smote)
cm = confusion_matrix(y_test, y_pred)
print(cm)
sns.heatmap(cm, annot=True)

"""## **Neural network**"""

#neural network
# import Adam
from keras.optimizers import Adam
import tensorflow as tf



Model1 = tf.keras.Sequential(
 [
 tf.keras.layers.Dense(32, activation="relu", input_shape=(X_train_smote.shape[-1],)),
 tf.keras.layers.Dense(128, activation="relu"),
 tf.keras.layers.Dropout(0.3),
 tf.keras.layers.Dense(256, activation="relu"),
 tf.keras.layers.Dropout(0.3),
 tf.keras.layers.Dense(1, activation="sigmoid"),
 ]
)

#Compile Model
Model1.compile(
 optimizer=tf.keras.optimizers.Adam(0.005),
 loss="binary_crossentropy",
 metrics=["acc"]
)

#Fit Model
history1 = Model1.fit(
 X_train_smote,
 y_train_smote,
 batch_size=2048,
 epochs=100,
 verbose=2,
 validation_split = 0.1)

nn_matrix = Model1.fit(X_train_smote, y_train_smote)
y_pred = Model1.predict(X_test_scaled)
y_pred_train = Model1.predict(X_train_smote)
y_pred_binary = np.where(y_pred >= 0.5, 1, 0)

# Calculer la matrice de confusion
cm = confusion_matrix(y_test, y_pred_binary)

print(cm)
sns.heatmap(cm, annot=True)

nn_matrix = Model1.fit(X_train_smote, y_train_smote)
y_pred = Model1.predict(X_test_scaled)
y_pred_train = Model1.predict(X_train_smote)
y_pred_binary = np.where(y_pred >= 0.5, 1, 0)

# Calculer la matrice de confusion
cr = classification_report(y_test, y_pred_binary)

print(cr)

# plot lossand accuracy of neural network
plt.plot(history1.history['loss'], label='train')
plt.plot(history1.history['val_loss'], label='test')
plt.legend()
plt.show()
plt.plot(history1.history['acc'], label='train')
plt.plot(history1.history['val_acc'], label='test')
plt.legend()
plt.show()

"""**The SHAP** (Shapley Additive Explanations) method is an interpretability technique for machine learning models that allows for understanding the predictions made by the model by assigning importance to each input variable.

More specifically, the SHAP method is based on game theory and Shapley values, which are used to quantify the contribution of each input variable to the model's prediction. It provides individual explanations for each data instance, which allow for understanding how the model makes decisions and how inputs have contributed to those decisions.
"""

import xgboost as xgb

print(xgb.__version__)

model = XGBClassifier(min_child_weight = 4,
                      gamma= 4,
                      subsample= 1.0,
                      colsample_bytree= 0.8,
                      max_depth = 4)
model.fit(X_train_smote,y_train_smote)

shap_values = shap.TreeExplainer(model).shap_values(X_train_smote)
shap.summary_plot(shap_values,X_train_smote)

explainer = shap.Explainer(model)
shap_values = explainer(X_test_scaled)
shap.plots.bar(shap_values)

import pickle
file = "try2.pkl"
pickle.dump(XGB_tune, open(file, 'wb'))
loaded_model = pickle.load(open(file, 'rb'))
pred_Y = loaded_model.predict(X_test_scaled)
result = np.round(accuracy_score(y_test, predict) ,2)
print(result)
dataset.info()